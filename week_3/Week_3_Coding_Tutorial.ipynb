{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Week 3 - Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zo5rD5ZcJDK_",
        "NrE0rpCVJDL1",
        "yrX43gwPJDL-",
        "MHcNqGnWJDMH",
        "9Ti4kMquJDML",
        "jR7y1e-xJDMd",
        "H3srEhqCJDM7"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk88Aw4NJDIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4121f9-c1a3-4305-fc62-b80ade5b37b4"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "print('GPU name: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n",
            "GPU name: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQb8nVqLJDI5"
      },
      "source": [
        "# Sequence modelling \n",
        "\n",
        "## Coding tutorials\n",
        " #### 1.  The IMDb dataset\n",
        " #### 2. Padding and masking sequence data\n",
        " #### 3. The `Embedding` layer\n",
        " #### 4. The Embedding Projector\n",
        " #### 5. Recurrent neural network layers\n",
        " #### 6. Stacked RNNs and the `Bidirectional` wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b3PNw3gJDI7"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## The IMDb Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwqGZQ8WJDJF"
      },
      "source": [
        "#### Load the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_c1LL5JDJG"
      },
      "source": [
        "# Import imdb\n",
        "\n",
        "import tensorflow.keras.datasets.imdb as imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jYJCwXJDJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114ccc15-bc9e-4f67-969f-48a6f3f14823"
      },
      "source": [
        "# Download and assign the data set using load_data()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oCnB8UMJDJP"
      },
      "source": [
        "#### Inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39RwIt-pJDJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7888d6-158d-43cf-bfa8-a04c4dc45574"
      },
      "source": [
        "# Inspect the type of the data\n",
        "\n",
        "type(x_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Viryy_PDJDJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b64d83-8303-4070-dc6d-5492b56d10a7"
      },
      "source": [
        "# Inspect the shape of the data\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q6E6v-FJDJX"
      },
      "source": [
        "# Display the first dataset element input\n",
        "# Notice encoding\n",
        "\n",
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbEqyjxJDJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89949da-dd04-4264-b1a0-90cc69eea906"
      },
      "source": [
        "# Display the first dataset element output\n",
        "\n",
        "y_train[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAk-PyOgJDJg"
      },
      "source": [
        "#### Load dataset with different options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfzFxDj-JDJh"
      },
      "source": [
        "# Load the dataset with defaults\n",
        "\n",
        "imdb.load_data(path='imdb.npz', index_from=3)\n",
        "\n",
        "# ~/.keras/dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuGfzZg3JDJv"
      },
      "source": [
        "# Limit the vocabulary to the top 500 words using num_words\n",
        "\n",
        "imdb.load_data(num_words=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMkXgUGaJDJy"
      },
      "source": [
        "# Ignore the top 10 most frequent words using skip_top\n",
        "\n",
        "imdb.load_data(skip_top=10, num_words=1000, oov_char=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDAFt0FJDJ3"
      },
      "source": [
        "# Limit the sequence lengths to 500 using maxlen\n",
        "\n",
        "imdb.load_data(maxlen=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI-Nr897LorW"
      },
      "source": [
        " # Use '1' as the character that indicates the start of a sequence\n",
        "\n",
        "imdb.load_data(start_char=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuBCKKGyJDJ7"
      },
      "source": [
        "#### Explore the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gkaFf6MJDJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11583890-3392-420e-e5bc-fa6c3636f3dc"
      },
      "source": [
        "# Load the imdb word index using get_word_index()\n",
        "\n",
        "imdb_word_index = imdb.get_word_index()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUzgAIoKJDKB"
      },
      "source": [
        "# View the word index as a dictionary,\n",
        "# accounting for index_from.\n",
        "\n",
        "index_from = 3\n",
        "imdb_word_index = {key: value + index_from for key, value in imdb_word_index.items()}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huCi_QIzJDKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599685d9-43ac-49d4-acd4-c8cc8f76032c"
      },
      "source": [
        "# Retrieve a specific word's index\n",
        "\n",
        "imdb_word_index['simpsonian']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h2tLklgm10l",
        "outputId": "d90bcc03-1408-46b1-d17d-a249632aa437"
      },
      "source": [
        "imdb_word_index['the']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7UkZwHiJDKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1ecd2e-5896-4c50-8203-69192dbec187"
      },
      "source": [
        "# View an input sentence\n",
        "\n",
        "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}\n",
        "[inv_imdb_word_index[index] for index in x_train[0] if index > index_from]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'film',\n",
              " 'was',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'casting',\n",
              " 'location',\n",
              " 'scenery',\n",
              " 'story',\n",
              " 'direction',\n",
              " \"everyone's\",\n",
              " 'really',\n",
              " 'suited',\n",
              " 'the',\n",
              " 'part',\n",
              " 'they',\n",
              " 'played',\n",
              " 'and',\n",
              " 'you',\n",
              " 'could',\n",
              " 'just',\n",
              " 'imagine',\n",
              " 'being',\n",
              " 'there',\n",
              " 'robert',\n",
              " \"redford's\",\n",
              " 'is',\n",
              " 'an',\n",
              " 'amazing',\n",
              " 'actor',\n",
              " 'and',\n",
              " 'now',\n",
              " 'the',\n",
              " 'same',\n",
              " 'being',\n",
              " 'director',\n",
              " \"norman's\",\n",
              " 'father',\n",
              " 'came',\n",
              " 'from',\n",
              " 'the',\n",
              " 'same',\n",
              " 'scottish',\n",
              " 'island',\n",
              " 'as',\n",
              " 'myself',\n",
              " 'so',\n",
              " 'i',\n",
              " 'loved',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'there',\n",
              " 'was',\n",
              " 'a',\n",
              " 'real',\n",
              " 'connection',\n",
              " 'with',\n",
              " 'this',\n",
              " 'film',\n",
              " 'the',\n",
              " 'witty',\n",
              " 'remarks',\n",
              " 'throughout',\n",
              " 'the',\n",
              " 'film',\n",
              " 'were',\n",
              " 'great',\n",
              " 'it',\n",
              " 'was',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'so',\n",
              " 'much',\n",
              " 'that',\n",
              " 'i',\n",
              " 'bought',\n",
              " 'the',\n",
              " 'film',\n",
              " 'as',\n",
              " 'soon',\n",
              " 'as',\n",
              " 'it',\n",
              " 'was',\n",
              " 'released',\n",
              " 'for',\n",
              " 'retail',\n",
              " 'and',\n",
              " 'would',\n",
              " 'recommend',\n",
              " 'it',\n",
              " 'to',\n",
              " 'everyone',\n",
              " 'to',\n",
              " 'watch',\n",
              " 'and',\n",
              " 'the',\n",
              " 'fly',\n",
              " 'fishing',\n",
              " 'was',\n",
              " 'amazing',\n",
              " 'really',\n",
              " 'cried',\n",
              " 'at',\n",
              " 'the',\n",
              " 'end',\n",
              " 'it',\n",
              " 'was',\n",
              " 'so',\n",
              " 'sad',\n",
              " 'and',\n",
              " 'you',\n",
              " 'know',\n",
              " 'what',\n",
              " 'they',\n",
              " 'say',\n",
              " 'if',\n",
              " 'you',\n",
              " 'cry',\n",
              " 'at',\n",
              " 'a',\n",
              " 'film',\n",
              " 'it',\n",
              " 'must',\n",
              " 'have',\n",
              " 'been',\n",
              " 'good',\n",
              " 'and',\n",
              " 'this',\n",
              " 'definitely',\n",
              " 'was',\n",
              " 'also',\n",
              " 'congratulations',\n",
              " 'to',\n",
              " 'the',\n",
              " 'two',\n",
              " 'little',\n",
              " \"boy's\",\n",
              " 'that',\n",
              " 'played',\n",
              " 'the',\n",
              " \"part's\",\n",
              " 'of',\n",
              " 'norman',\n",
              " 'and',\n",
              " 'paul',\n",
              " 'they',\n",
              " 'were',\n",
              " 'just',\n",
              " 'brilliant',\n",
              " 'children',\n",
              " 'are',\n",
              " 'often',\n",
              " 'left',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'praising',\n",
              " 'list',\n",
              " 'i',\n",
              " 'think',\n",
              " 'because',\n",
              " 'the',\n",
              " 'stars',\n",
              " 'that',\n",
              " 'play',\n",
              " 'them',\n",
              " 'all',\n",
              " 'grown',\n",
              " 'up',\n",
              " 'are',\n",
              " 'such',\n",
              " 'a',\n",
              " 'big',\n",
              " 'profile',\n",
              " 'for',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'film',\n",
              " 'but',\n",
              " 'these',\n",
              " 'children',\n",
              " 'are',\n",
              " 'amazing',\n",
              " 'and',\n",
              " 'should',\n",
              " 'be',\n",
              " 'praised',\n",
              " 'for',\n",
              " 'what',\n",
              " 'they',\n",
              " 'have',\n",
              " 'done',\n",
              " \"don't\",\n",
              " 'you',\n",
              " 'think',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'story',\n",
              " 'was',\n",
              " 'so',\n",
              " 'lovely',\n",
              " 'because',\n",
              " 'it',\n",
              " 'was',\n",
              " 'true',\n",
              " 'and',\n",
              " 'was',\n",
              " \"someone's\",\n",
              " 'life',\n",
              " 'after',\n",
              " 'all',\n",
              " 'that',\n",
              " 'was',\n",
              " 'shared',\n",
              " 'with',\n",
              " 'us',\n",
              " 'all']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq2ID8wRJDKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b79218-9239-4b48-9f5f-875f8521d3c9"
      },
      "source": [
        "# Get the sentiment value\n",
        "\n",
        "y_train[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcPUtmz-JDKZ"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Padding and Masking Sequence Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UggNd-8VLgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb38308-840e-4639-b88f-9a8ecc265d47"
      },
      "source": [
        "# Load the imdb data set\n",
        "\n",
        "import tensorflow.keras.datasets.imdb as imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuqWhXi-JDKa"
      },
      "source": [
        "#### Preprocess the data with padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWULtJ7CJDKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2991ea7f-3f44-420a-c13b-4a402b99ff9e"
      },
      "source": [
        "# Inspect the input data shape\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOts_k01JDKh"
      },
      "source": [
        "# Pad the inputs to the maximum length using maxlen\n",
        "\n",
        "padded_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=300, padding='post', truncating='pre')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPiMGwDJDKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a3822e-9c7c-4140-d23c-7d02f975ea73"
      },
      "source": [
        "# Inspect the output data shape\n",
        "\n",
        "padded_x_train.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJt56letJDKn"
      },
      "source": [
        "#### Create a Masking layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoMdifYJDKo"
      },
      "source": [
        "# Import numpy \n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8LjX9QaJDKr"
      },
      "source": [
        "# Masking expects to see (batch, sequence, features)\n",
        "# Create a dummy feature dimension using expand_dims\n",
        "\n",
        "padded_x_train = np.expand_dims(padded_x_train, -1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NJdAyltpVHv",
        "outputId": "7d53a6a7-bc59-4da9-92ab-48c25297eec3"
      },
      "source": [
        "padded_x_train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFrgXbDrJDKt"
      },
      "source": [
        "# Create a Masking layer \n",
        "\n",
        "tf_x_train = tf.convert_to_tensor(padded_x_train, dtype='float32')\n",
        "masking_layer = tf.keras.layers.Masking(mask_value=0.0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkSzdHwJDKw"
      },
      "source": [
        "# Pass tf_x_train to it\n",
        "\n",
        "masked_x_train = masking_layer(tf_x_train)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuStn9s0JDK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5edc16-0fe3-451d-d6dd-2a6f020b7c17"
      },
      "source": [
        "# Look at the dataset\n",
        "\n",
        "masked_x_train[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(300, 1), dtype=float32, numpy=\n",
              "array([[1.0000e+00],\n",
              "       [1.4000e+01],\n",
              "       [2.2000e+01],\n",
              "       [1.6000e+01],\n",
              "       [4.3000e+01],\n",
              "       [5.3000e+02],\n",
              "       [9.7300e+02],\n",
              "       [1.6220e+03],\n",
              "       [1.3850e+03],\n",
              "       [6.5000e+01],\n",
              "       [4.5800e+02],\n",
              "       [4.4680e+03],\n",
              "       [6.6000e+01],\n",
              "       [3.9410e+03],\n",
              "       [4.0000e+00],\n",
              "       [1.7300e+02],\n",
              "       [3.6000e+01],\n",
              "       [2.5600e+02],\n",
              "       [5.0000e+00],\n",
              "       [2.5000e+01],\n",
              "       [1.0000e+02],\n",
              "       [4.3000e+01],\n",
              "       [8.3800e+02],\n",
              "       [1.1200e+02],\n",
              "       [5.0000e+01],\n",
              "       [6.7000e+02],\n",
              "       [2.2665e+04],\n",
              "       [9.0000e+00],\n",
              "       [3.5000e+01],\n",
              "       [4.8000e+02],\n",
              "       [2.8400e+02],\n",
              "       [5.0000e+00],\n",
              "       [1.5000e+02],\n",
              "       [4.0000e+00],\n",
              "       [1.7200e+02],\n",
              "       [1.1200e+02],\n",
              "       [1.6700e+02],\n",
              "       [2.1631e+04],\n",
              "       [3.3600e+02],\n",
              "       [3.8500e+02],\n",
              "       [3.9000e+01],\n",
              "       [4.0000e+00],\n",
              "       [1.7200e+02],\n",
              "       [4.5360e+03],\n",
              "       [1.1110e+03],\n",
              "       [1.7000e+01],\n",
              "       [5.4600e+02],\n",
              "       [3.8000e+01],\n",
              "       [1.3000e+01],\n",
              "       [4.4700e+02],\n",
              "       [4.0000e+00],\n",
              "       [1.9200e+02],\n",
              "       [5.0000e+01],\n",
              "       [1.6000e+01],\n",
              "       [6.0000e+00],\n",
              "       [1.4700e+02],\n",
              "       [2.0250e+03],\n",
              "       [1.9000e+01],\n",
              "       [1.4000e+01],\n",
              "       [2.2000e+01],\n",
              "       [4.0000e+00],\n",
              "       [1.9200e+03],\n",
              "       [4.6130e+03],\n",
              "       [4.6900e+02],\n",
              "       [4.0000e+00],\n",
              "       [2.2000e+01],\n",
              "       [7.1000e+01],\n",
              "       [8.7000e+01],\n",
              "       [1.2000e+01],\n",
              "       [1.6000e+01],\n",
              "       [4.3000e+01],\n",
              "       [5.3000e+02],\n",
              "       [3.8000e+01],\n",
              "       [7.6000e+01],\n",
              "       [1.5000e+01],\n",
              "       [1.3000e+01],\n",
              "       [1.2470e+03],\n",
              "       [4.0000e+00],\n",
              "       [2.2000e+01],\n",
              "       [1.7000e+01],\n",
              "       [5.1500e+02],\n",
              "       [1.7000e+01],\n",
              "       [1.2000e+01],\n",
              "       [1.6000e+01],\n",
              "       [6.2600e+02],\n",
              "       [1.8000e+01],\n",
              "       [1.9193e+04],\n",
              "       [5.0000e+00],\n",
              "       [6.2000e+01],\n",
              "       [3.8600e+02],\n",
              "       [1.2000e+01],\n",
              "       [8.0000e+00],\n",
              "       [3.1600e+02],\n",
              "       [8.0000e+00],\n",
              "       [1.0600e+02],\n",
              "       [5.0000e+00],\n",
              "       [4.0000e+00],\n",
              "       [2.2230e+03],\n",
              "       [5.2440e+03],\n",
              "       [1.6000e+01],\n",
              "       [4.8000e+02],\n",
              "       [6.6000e+01],\n",
              "       [3.7850e+03],\n",
              "       [3.3000e+01],\n",
              "       [4.0000e+00],\n",
              "       [1.3000e+02],\n",
              "       [1.2000e+01],\n",
              "       [1.6000e+01],\n",
              "       [3.8000e+01],\n",
              "       [6.1900e+02],\n",
              "       [5.0000e+00],\n",
              "       [2.5000e+01],\n",
              "       [1.2400e+02],\n",
              "       [5.1000e+01],\n",
              "       [3.6000e+01],\n",
              "       [1.3500e+02],\n",
              "       [4.8000e+01],\n",
              "       [2.5000e+01],\n",
              "       [1.4150e+03],\n",
              "       [3.3000e+01],\n",
              "       [6.0000e+00],\n",
              "       [2.2000e+01],\n",
              "       [1.2000e+01],\n",
              "       [2.1500e+02],\n",
              "       [2.8000e+01],\n",
              "       [7.7000e+01],\n",
              "       [5.2000e+01],\n",
              "       [5.0000e+00],\n",
              "       [1.4000e+01],\n",
              "       [4.0700e+02],\n",
              "       [1.6000e+01],\n",
              "       [8.2000e+01],\n",
              "       [1.0311e+04],\n",
              "       [8.0000e+00],\n",
              "       [4.0000e+00],\n",
              "       [1.0700e+02],\n",
              "       [1.1700e+02],\n",
              "       [5.9520e+03],\n",
              "       [1.5000e+01],\n",
              "       [2.5600e+02],\n",
              "       [4.0000e+00],\n",
              "       [3.1050e+04],\n",
              "       [7.0000e+00],\n",
              "       [3.7660e+03],\n",
              "       [5.0000e+00],\n",
              "       [7.2300e+02],\n",
              "       [3.6000e+01],\n",
              "       [7.1000e+01],\n",
              "       [4.3000e+01],\n",
              "       [5.3000e+02],\n",
              "       [4.7600e+02],\n",
              "       [2.6000e+01],\n",
              "       [4.0000e+02],\n",
              "       [3.1700e+02],\n",
              "       [4.6000e+01],\n",
              "       [7.0000e+00],\n",
              "       [4.0000e+00],\n",
              "       [1.2118e+04],\n",
              "       [1.0290e+03],\n",
              "       [1.3000e+01],\n",
              "       [1.0400e+02],\n",
              "       [8.8000e+01],\n",
              "       [4.0000e+00],\n",
              "       [3.8100e+02],\n",
              "       [1.5000e+01],\n",
              "       [2.9700e+02],\n",
              "       [9.8000e+01],\n",
              "       [3.2000e+01],\n",
              "       [2.0710e+03],\n",
              "       [5.6000e+01],\n",
              "       [2.6000e+01],\n",
              "       [1.4100e+02],\n",
              "       [6.0000e+00],\n",
              "       [1.9400e+02],\n",
              "       [7.4860e+03],\n",
              "       [1.8000e+01],\n",
              "       [4.0000e+00],\n",
              "       [2.2600e+02],\n",
              "       [2.2000e+01],\n",
              "       [2.1000e+01],\n",
              "       [1.3400e+02],\n",
              "       [4.7600e+02],\n",
              "       [2.6000e+01],\n",
              "       [4.8000e+02],\n",
              "       [5.0000e+00],\n",
              "       [1.4400e+02],\n",
              "       [3.0000e+01],\n",
              "       [5.5350e+03],\n",
              "       [1.8000e+01],\n",
              "       [5.1000e+01],\n",
              "       [3.6000e+01],\n",
              "       [2.8000e+01],\n",
              "       [2.2400e+02],\n",
              "       [9.2000e+01],\n",
              "       [2.5000e+01],\n",
              "       [1.0400e+02],\n",
              "       [4.0000e+00],\n",
              "       [2.2600e+02],\n",
              "       [6.5000e+01],\n",
              "       [1.6000e+01],\n",
              "       [3.8000e+01],\n",
              "       [1.3340e+03],\n",
              "       [8.8000e+01],\n",
              "       [1.2000e+01],\n",
              "       [1.6000e+01],\n",
              "       [2.8300e+02],\n",
              "       [5.0000e+00],\n",
              "       [1.6000e+01],\n",
              "       [4.4720e+03],\n",
              "       [1.1300e+02],\n",
              "       [1.0300e+02],\n",
              "       [3.2000e+01],\n",
              "       [1.5000e+01],\n",
              "       [1.6000e+01],\n",
              "       [5.3450e+03],\n",
              "       [1.9000e+01],\n",
              "       [1.7800e+02],\n",
              "       [3.2000e+01],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00],\n",
              "       [0.0000e+00]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O54DVLx4JDK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4edb772-b34c-4be4-d4e2-e144c32d2ae5"
      },
      "source": [
        "# Look at the ._keras_mask for the dataset\n",
        "\n",
        "masked_x_train._keras_mask"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5rD5ZcJDK_"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## The Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkBOM8mJDLA"
      },
      "source": [
        "#### Create and apply an `Embedding` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-esPcdDJDLJ"
      },
      "source": [
        "# Create an embedding layer using layers.Embedding\n",
        "# Specify input_dim, output_dim, input_length\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf3B6HamJDLN"
      },
      "source": [
        "# Inspect an Embedding layer output for a fixed input\n",
        "# Expects an input of shape (batch, sequence, feature)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ualmsaPpJDLV"
      },
      "source": [
        "# Inspect the Embedding layer weights using get_weights()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wadlt3AJDLX"
      },
      "source": [
        "# Get the embedding for the 14th index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFuhReOm9xF7"
      },
      "source": [
        "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKYwKT_I-H2O"
      },
      "source": [
        "# Create a layer that uses the mask_zero kwarg\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hc1zx6A-H6R"
      },
      "source": [
        "# Apply this layer to the sequence and see the _keras_mask property\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUG6LF1MJDL0"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Embedding Projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zna3TCoTAu00"
      },
      "source": [
        "#### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPKYrlepAxPV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBAX9ENDBFFE"
      },
      "source": [
        "#### Load and preprocess the IMDb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo5qBbDDBIdn"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGpymnb2BIiR"
      },
      "source": [
        "# Load the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EtU2vK0BLts"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtkIEIdRBSxv"
      },
      "source": [
        "# Get the word index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur2fp10YBS6T"
      },
      "source": [
        "# Swap the keys and values of the word index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cquirCA8BS99"
      },
      "source": [
        "# View the first dataset example sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrE0rpCVJDL1"
      },
      "source": [
        "#### Build an Embedding layer into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPUfv9kjJDL1"
      },
      "source": [
        "# Get the maximum token value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO0CkecjJDL5"
      },
      "source": [
        "# Specify an embedding dimension\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFBZOlp3JDL7"
      },
      "source": [
        "# Build a model using Sequential:\n",
        "#     1. Embedding layer\n",
        "#     2. GlobalAveragePooling1D\n",
        "#     3. Dense\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw9qhtlPJDL9"
      },
      "source": [
        "# Functional API refresher: use the Model to build the same model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf6oaEvTCKxV"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrX43gwPJDL-"
      },
      "source": [
        "#### Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVfI_1EoJDL_"
      },
      "source": [
        "# Compile the model with a binary cross-entropy loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvR-O7wGJDMC"
      },
      "source": [
        "# Train the model using .fit(), savng its history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ind0d_gvJDMG"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcNqGnWJDMH"
      },
      "source": [
        "#### The TensorFlow embedding projector\n",
        "\n",
        "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVP4G9X0JDMH"
      },
      "source": [
        "# Retrieve the embedding layer's weights from the trained model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHd29nfZJDMJ"
      },
      "source": [
        "# Save the word Embeddings to tsv files\n",
        "# Two files: \n",
        "#     one contains the embedding labels (meta.tsv),\n",
        "#     one contains the embeddings (vecs.tsv)\n",
        "\n",
        "import io\n",
        "from os import path\n",
        "\n",
        "out_v = io.open(path.join('data', 'vecs.tsv'), 'w', encoding='utf-8')\n",
        "out_m = io.open(path.join('data', 'meta.tsv'), 'w', encoding='utf-8')\n",
        "\n",
        "k = 0\n",
        "\n",
        "for word, token in word_index.items():\n",
        "    if k != 0:\n",
        "        out_m.write('\\n')\n",
        "        out_v.write('\\n')\n",
        "    \n",
        "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
        "    out_m.write(word)\n",
        "    k += 1\n",
        "    \n",
        "out_v.close()\n",
        "out_m.close()\n",
        "# beware large collections of embeddings!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ti4kMquJDML"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Recurrent neural network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hrm6Q2jJDMM"
      },
      "source": [
        "#### Initialize and pass an input to a SimpleRNN layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM7uSwkQJDMR"
      },
      "source": [
        "# Create a SimpleRNN layer and test it\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_YJUyrEJDMT"
      },
      "source": [
        "# Note that only the final cell output is returned\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_rwdZtmJDMV"
      },
      "source": [
        "#### Load and transform the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvAtycIpH1aA"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rJuSFmJDMV"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwUHcFKwH4wh"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfyqmfOXJDMX"
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR7y1e-xJDMd"
      },
      "source": [
        "#### Create a recurrent neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tym76m2dIVOZ"
      },
      "source": [
        "# Get the maximum index value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tO953-oJDMd"
      },
      "source": [
        "# Using Sequential, build the model:\n",
        "# 1. Embedding.\n",
        "# 2. LSTM.\n",
        "# 3. Dense.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v076l5CUJDMf"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRRXW5mPJDMg"
      },
      "source": [
        "# Compile the model with binary cross-entropy loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "216PeHZFJDMi"
      },
      "source": [
        "# Fit the model and save its training history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVXF1TSJDMj"
      },
      "source": [
        "#### Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms9VW07lJDMk"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AAyEd6wJDMm"
      },
      "source": [
        "#### Make predictions with the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk9CHYLiJDMo"
      },
      "source": [
        "# View the first test data example sentence\n",
        "# (invert the word index)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blf6in2dJDMs"
      },
      "source": [
        "# Get the model prediction using model.predict()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCyKBsZHJDMt"
      },
      "source": [
        "# Get the corresponding label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpaI83PlJDMv"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_6\"></a>\n",
        "## Stacked RNNs and the Bidirectional wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qr4kOevKRt8"
      },
      "source": [
        "#### Load and transform the IMDb review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i06LdJiXKRt9"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjA8JlQVKRuB"
      },
      "source": [
        "# Load the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBFGx9_KRuD"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29lcV0UGKRuF"
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh_Vv9-5JDM1"
      },
      "source": [
        "#### Build stacked and bidirectional recurrent models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Sy-gMyLLEI"
      },
      "source": [
        "# Get the maximum index value and specify an embedding dimension\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89yWIAFdJDM1"
      },
      "source": [
        "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-34ZWvRJDM3"
      },
      "source": [
        "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSpOIOCmJDM4"
      },
      "source": [
        "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3srEhqCJDM7"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Dy_C6-JDM7"
      },
      "source": [
        "# Compile the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er8atiBoJDM9"
      },
      "source": [
        "# Train the model, saving its history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLOLtBKwJDNA"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}